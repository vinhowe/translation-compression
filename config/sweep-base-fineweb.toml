[data]
train_bin = "data/fineweb350B-dedup-suffix-31/fineweb350b-dedup_train_*.bin"
val_bin = "data/fineweb350B-dedup-suffix-31/fineweb350b-dedup_val_*.bin"

[model]
block_size = 64
vocab_size = 151936
weight_tying = false
# size_tier set via sweep; n_layer/n_embd/n_head derived; batch/accum derived

[training]
max_iters = 1e6
eval_interval = 1000
eval_iters = 1
log_interval = 10
always_save_checkpoints = true
# batch_size and gradient_accumulation_steps set by size_tier
# seed set via sweep

[optimizer]
learning_rate = 2e-5
weight_decay = 0

[system]
compile = true
dtype = "bfloat16"

[logging]
wandb_log = true
wandb_project = "translation-compression"
wandb_run_name = "sweep"
wandb_notes = "Seed-64 sweep prototype 1"
wandb_group = "seed-64-sweep-prototype-1"

[experiment]
# Should redo this so that it's experimental configs instead of this many-degrees-of-freedom thing
n_compartments = 1
compartment_scaling = "equal"
translation_ratio = 0
max_compartments = 128
# assignment_seed mirrors training.seed in code
use_compartment_embeddings = true
permute_tokens_per_compartment = true



